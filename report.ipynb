{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5065488f",
   "metadata": {},
   "source": [
    "## Poking around the World State\n",
    "\n",
    "To train a robotic hand to interact with the physical world, we require a reliable 3D reconstruction of the scene. Therefore, I focused on these three attribute and listed their corresponding reason why I found them vital.\n",
    "\n",
    "1. **Hand Pose Keypoints**: They are essential for us to reconstruct hand geometry and articulation. In other words, this help us \"build a hand\" in our recontrusction. \n",
    "2. **Depth Estimation**: TIn addition to hand pose keypoints, we still need to localize the hand and surrounding environment in 3D space, depth estimation will be of great importance in this stage.\n",
    "3. **Object Detection**: Due to time limit, I didn't explore this area in detail. Howeveer, I can tell thhat it will help us identify objects and potentially attach semantic or physical properties, enabling reasoning about hand–object interactions and future extensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757579f",
   "metadata": {},
   "source": [
    "### Handpose Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7f7ee",
   "metadata": {},
   "source": [
    "We use MediaPipe to perform hand pose tracking and implement a HandposeMarker class that supports annotating hand keypoints and skeletal connections on both images and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772c92cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRIPT_DIR: d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\n",
      "IMG_PATH: d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\\..\\src\\imgs\\hand.jpg\n",
      "VIDEO_PATH: d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\\..\\src\\videos\\1.mp4\n",
      "Annotated image saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 171/171 [00:13<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handpose video saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# demo\n",
    "from scripts.HandposeMarker import HandposeMarker\n",
    "from scripts.HandposeMarker import demo as handpose_demo\n",
    "\n",
    "def main():\n",
    "    handpose_demo()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d9338",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/hand.jpg\" width=\"50%\">\n",
    "    <img src=\"src/imgs/hand_handpose.jpg\" width=\"50%\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0ef94",
   "metadata": {},
   "source": [
    "A major limitation we observe is that hand pose estimation becomes unreliable under occlusion, where self-occlusion or object occlusion often leads to inaccurate keypoints or missing the entire hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87073f",
   "metadata": {},
   "source": [
    "### Depth Estimation (Monocular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498d23d",
   "metadata": {},
   "source": [
    "We investigate both relative depth estimation with Depth Anything and absolute depth estimation with UniDepth, but face practical deployment challenges with UniDepth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e9ef1",
   "metadata": {},
   "source": [
    "#### DepthAnything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfb81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb958e0e7774ab98052ce3b3e68e541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\\..\\src\\imgs\\cat.png\n",
      "Depth map saved to d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\\..\\src\\imgs\\cat_relative_depth.png\n"
     ]
    }
   ],
   "source": [
    "from scripts.DepthAnythingDemo import main as depth_anything_demo\n",
    "depth_anything_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8a176",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/cat.png\" width=\"50%\">\n",
    "    <img src=\"src/imgs/cat_relative_depth.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925ac06",
   "metadata": {},
   "source": [
    "The model produces reasonable relative depth maps that capture coarse scene geometry, but still exhibits limitations in fine-grained detail, and its performance is subject ot both lighting condition and the scale of model used. \n",
    "\n",
    "To illustrate this effect, we present a simple, non-rigorous experiment showing how additional illumination influences relative depth predictions.\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/img_a.jpg\" width=\"50%\">\n",
    "    <img src=\"src/imgs/depth_a.png\" width=\"50%\">\n",
    "</div>\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/img_b.jpg\" width=\"50%\">\n",
    "    <img src=\"src/imgs/depth_b.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4e0b0",
   "metadata": {},
   "source": [
    "#### Unidepth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2d3a4",
   "metadata": {},
   "source": [
    "Although UniDepth V2 is well-suited for metric depth estimation, practical deployment is hindered by dependency and toolchain issues in our current Windows-based environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc3344",
   "metadata": {},
   "source": [
    "When attempting to use UniDepthV2 for depth estimation, I initially overlooked its implicit assumptions about the dependencies. This project is primarily designed for Linux platforms and relies on a series of CUDA extensions and third-party high-performance operator libraries. When trying to use it on Windows 11, I had to work out a combination of Visual Studio, CUDA Toolkit, and related libary toolchains to meet these dependencies, which introduced significant environmental complexity and triggered some system-level conflicts. For example, it had a negative impact on my Monogame development environment, and I took trouble addressing it. Furthermore, inspired by my work on object detection, I introduced Cupy to take to place of Numpy, so as to reduce CPU-GPU data transfer latency. However, this further increased debugging and maintenance costs on this platform. After a comprehensive evaluation, I chose to abandon this approach and revert to a more stable and controllable implementation. To sum up, the challenges stemmed from not the model itself, but the mismatch between software ecosystem and rapidly evolving hardware architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d84e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not loading pretrained weights for backbone\n",
      "EdgeGuidedLocalSSI reverts to a non cuda-optimized operation, you will experince large slowdown, please install it:  `cd ./unidepth/ops/extract_patches && bash compile.sh`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UniDepth\\unidepth\\models\\unidepthv2\\unidepthv2.py:262: UserWarning: !! self.resolution_level not set, using default bounds !!\n",
      "  warnings.warn(\"!! self.resolution_level not set, using default bounds !!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved demo to d:\\NYU_Files\\2026_Spring\\summer_research\\NYU UGSRP\\EMBODY_INTELL_EXPORT\\scripts\\..\\src\\imgs/hand_unidepth.png\n"
     ]
    }
   ],
   "source": [
    "# be sure to install Unidepth into your environment!\n",
    "from scripts.UniDepthMarker import demo as unidepth_demo\n",
    "unidepth_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071afad7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/hand.jpg\" width=\"50%\">\n",
    "    <img src=\"src/imgs/hand_unidepth.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfafcd",
   "metadata": {},
   "source": [
    "Due to environment-related issues, part of the time was spent on setup and debugging. At the current stage, we are able to successfully visualize depth results. However, we hope that in the near future, this module can be combined with the previously implemented hand pose marker to produce records of hand keypoints moving in 3D space over time. Our goal is to reproduce a dataset that is similar to provided hdf5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ecd84",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53abb97",
   "metadata": {},
   "source": [
    "The object detection module is based on a YOLO model, but its current performance of both quality and efficiency is unsatisfactory. The exact causes have not yet been fully investigated, though the issue may be related to the model scale. At this stage, the current detection quality may be insufficient to reliably support downstream training for embodied intelligence tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4aebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ObjDetector import main as obj_detector_demo\n",
    "obj_detector_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6faab",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "    <img src=\"src/imgs/hand.jpg\" width=\"50%\">\n",
    "    <img src=\"src/imgs/hand_obj_detection.png\" width=\"50%\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unidepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
